<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>NSF/FDA SIR: System Solutions for Improved Image Quality of Context-aware Mobile Displays</AwardTitle>
    <AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>105000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07020000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Chem, Bioeng, Env, &amp; Transp Sys</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Leon Esterowitz</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>1136944&lt;br/&gt;Zhong&lt;br/&gt;&lt;br/&gt;Unlike PCs, mobile devices such as smart-phones and tablets are likely to be used under various viewing conditions. Many contextual factors including viewing angle, ambient light, and a shaking hand will introduce significant distortion in the perceived image quality, challenging the use of mobile devices to view medical images. On February 4, 2011, the US FDA cleared the first diagnostic radiology application that allows the physicians to view CT, MR, and PET images on the iPhone and iPad mobile devices when there is no access to a workstation. While this ruling enables physicians to view images and make diagnoses under recommended lighting conditions, it mandates that the cleared application include labeling and safety features to mitigate the risk of adverse viewing conditions. The challenge from the contextual factors remains unaddressed: the physicians are responsible to operate a mobile display in a proper viewing environment. &lt;br/&gt;&lt;br/&gt;The goal of this research project is to address the challenge to mobile image quality from contextual factors with a computational and sensing approach. The key point is that efficient sensing and computational power of mobile devices can be leveraged to extract information about the viewing context to compensate for image distortions in mobile display. The proposed research will heavily draw upon the modern understanding of the human visual system. It will not only offer solutions that are applicable to all kinds of mobile displays but also will provide technology-specific solutions for existing and emerging mobile display technologies, from liquid-crystal display (LCD) to organic light-emitting diode (OLED) to auto-stereoscopic three-dimensional (3D) technologies.</AbstractNarration>
    <MinAmdLetterDate>05/10/2011</MinAmdLetterDate>
    <MaxAmdLetterDate>05/10/2011</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1136944</AwardID>
    <Investigator>
      <FirstName>Lin</FirstName>
      <LastName>Zhong</LastName>
      <EmailAddress>LZhong@rice.edu</EmailAddress>
      <StartDate>05/10/2011</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>William Marsh Rice University</Name>
      <CityName>Houston</CityName>
      <ZipCode>770051827</ZipCode>
      <PhoneNumber>7133484820</PhoneNumber>
      <StreetAddress>6100 MAIN ST</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7236</Code>
      <Text>BIOPHOTONICS, IMAGING &amp;SENSING</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>005E</Code>
      <Text>Neuro-photonics</Text>
    </ProgramReference>
  </Award>
</rootTag>

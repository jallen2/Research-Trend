<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>I-Corps: Semantic Video - from Video to Descriptions</AwardTitle>
    <AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2017</AwardExpirationDate>
    <AwardAmount>50000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Steven Konsek</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The broader impact/commercial potential of this I-Corps project involves computer vision analysis of video, using both visual and auditory cues, to create descriptions of the content. The technology has a large variety of potential applications from law enforcement to surveillance to consumer applications. These include enabling the efficient storage and retrieval of large volumes of camera data. Smart surveillance systems can be enhanced with features that allows for summarization of daylong video footages as a list of security-relevant events. The technology can also allow automated organization of large collections of multimedia data.&lt;br/&gt;&lt;br/&gt;This I-Corps project involves commercialization feasibility research for a computer vision technology for expressing video content in terms of natural language text and grammar, i.e. semantics. This project builds on a video analysis framework that leverages state-of-the-art methods for object detection and action recognition in a unified formalism encoded in terms of a mathematical and statistical approach known as pattern theory. The video analysis approach can (i) handle structural variability of complex events without requiring large training data while exploiting easily available ontological information, (ii) overcome classification errors of machine learning classifiers of actions and objects, (iii) accommodate scene clutter, i.e. extraneous objects that do not in the activity present in the scene, (iv) and manage sequences of elementary events, all without retraining. The formalism allows for the easy incorporation of temporal, spatial, and logical constraints. This team has demonstrated this system on standard datasets used to benchmark performance in computer vision for human activity recognition tasks.</AbstractNarration>
    <MinAmdLetterDate>08/09/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>08/09/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1647887</AwardID>
    <Investigator>
      <FirstName>Sudeep</FirstName>
      <LastName>Sarkar</LastName>
      <EmailAddress>sarkar@cse.usf.edu</EmailAddress>
      <StartDate>08/09/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of South Florida</Name>
      <CityName>Tampa</CityName>
      <ZipCode>336129446</ZipCode>
      <PhoneNumber>8139742897</PhoneNumber>
      <StreetAddress>3702 Spectrum Blvd.</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Florida</StateName>
      <StateCode>FL</StateCode>
    </Institution>
    <ProgramElement>
      <Code>8023</Code>
      <Text>I-Corps</Text>
    </ProgramElement>
  </Award>
</rootTag>

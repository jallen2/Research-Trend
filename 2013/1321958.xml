<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>DC: Small: Collaborative Research: DARE: Declarative and Scalable Recovery</AwardTitle>
    <AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>235663</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Hong Jiang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>One dominant characteristic of today's large-scale computing systems&lt;br/&gt;is the prevalence of large storage clusters. Storage clusters at the&lt;br/&gt;scale of hundreds or thousands of commodity machines are&lt;br/&gt;increasingly being deployed. At companies like Amazon, Google, Yahoo,&lt;br/&gt;and others, thousands of nodes are managed as a single system.&lt;br/&gt;&lt;br/&gt;As large clusters have brought many benefits, they also bring a new&lt;br/&gt;challenge: a growing number and frequency of failures that must be&lt;br/&gt;managed. Bits, sectors, disks, machines, racks, and many other&lt;br/&gt;components fail. With millions of servers and hundreds of data&lt;br/&gt;centers, there are millions of opportunities for these components to&lt;br/&gt;fail. Failing to deal with failures will directly impact the&lt;br/&gt;reliability and availability of data and jobs.&lt;br/&gt;&lt;br/&gt;Unfortunately, we still hear data-loss stories even recently. For&lt;br/&gt;example, in March 2009, Facebook lost millions of photos due to&lt;br/&gt;simultaneous disk failures that "should" rarely happen at the same&lt;br/&gt;time (but it happened); in July 2009, a large bank was fined a record&lt;br/&gt;total of 3 millions pounds after losing data on thousands of its&lt;br/&gt;customers; more recently, in October 2009, T-Mobile Sidekick, which&lt;br/&gt;uses Microsoft's cloud service, also lost its customer data. These&lt;br/&gt;incidents have shown that existing large-scale storage systems are&lt;br/&gt;still fragile to failures.&lt;br/&gt;&lt;br/&gt;To address the challenges of large-scale recovery, the goal of this&lt;br/&gt;project is to: (1) seek the fundamental problems of recovery in&lt;br/&gt;today's scalable world of computing, (2) improve the reliability,&lt;br/&gt;performance, and scalability of existing large-scale recovery, and (3)&lt;br/&gt;explore formally grounded languages to empower rigorous specification&lt;br/&gt;of recovery properties and behaviors. Our vision is to build systems&lt;br/&gt;that "DARE to fail": systems that deliberately fail themselves,&lt;br/&gt;exercise recovery routinely, and enable easy and correct deployment of&lt;br/&gt;new recovery policies.&lt;br/&gt;&lt;br/&gt;For more information, please visit this website:&lt;br/&gt;http://boom.cs.berkeley.edu/dare/</AbstractNarration>
    <MinAmdLetterDate>04/09/2013</MinAmdLetterDate>
    <MaxAmdLetterDate>09/13/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1321958</AwardID>
    <Investigator>
      <FirstName>Haryadi</FirstName>
      <LastName>Gunawi</LastName>
      <EmailAddress>haryadi@cs.uchicago.edu</EmailAddress>
      <StartDate>04/09/2013</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Chicago</Name>
      <CityName>Chicago</CityName>
      <ZipCode>606375418</ZipCode>
      <PhoneNumber>7737028669</PhoneNumber>
      <StreetAddress>5801 South Ellis Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Illinois</StateName>
      <StateCode>IL</StateCode>
    </Institution>
  </Award>
</rootTag>

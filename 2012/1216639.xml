<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Collaborative Research: Dynamics of Interpersonal Coordination and Embodied Communication</AwardTitle>
    <AwardEffectiveDate>11/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2013</AwardExpirationDate>
    <AwardAmount>39880</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Betty H. Tuller</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Communication is an act of coordination. Conversants must coordinate their mental activities to achieve the common ground required for effective communication. But how is this coordination achieved? This project tests the hypothesis that coordination of body movements and eye movements helps minds work together. The objective is to evaluate whether coordinated movement that occurs during conversation embodies the coordinated mental activities that make communication possible. The research employs a highly innovative approach to understanding these processes of mental and bodily coordination. The approach capitalizes on methods derived from nonlinear dynamical systems theory, a sub-domain of complexity science. The investigators will record patterns of body movements, such as gestures and postures, and patterns of eye movements. They will then apply time series analyses designed to capture the complexity of nonlinear dynamical systems to quantify the coordination of body and eye movements between people who are conversing.&lt;br/&gt;&lt;br/&gt;Research on interpersonal coordination during conversation is a theoretically important and far-reaching issue because it serves as a hub topic for interdisciplinary studies of cognition, perception-action, language and communication, social psychology, and neuroscience. The proposed research offers innovative methods for studying joint action and strategies for understanding the relation between coordinated movement and coordinated cognition during joint action. These new measures of coordination provide quantitative tools for studying the complex and covert processes of conversational interaction. As part of this project, the investigators will establish a freely available, online, Joint Action Research Database (JARD). This will enhance the research infrastructure in this interdisciplinary field by serving as a repository for eye and body movement time series and speech data collected in this project and from other interested labs. In parallel with JARD the researchers will make available the software analysis tools appropriate for studying the complex dynamics of interpersonal coordination.</AbstractNarration>
    <MinAmdLetterDate>01/05/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>01/05/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1216639</AwardID>
    <Investigator>
      <FirstName>Rick</FirstName>
      <LastName>Dale</LastName>
      <EmailAddress>rdale@ucmerced.edu</EmailAddress>
      <StartDate>01/05/2012</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California - Merced</Name>
      <CityName>Merced</CityName>
      <ZipCode>953435001</ZipCode>
      <PhoneNumber>2097566405</PhoneNumber>
      <StreetAddress>5200 North Lake Road</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9150</Code>
      <Text>EXP PROG TO STIM COMP RES</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>

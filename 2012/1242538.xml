<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>I-Corps: ZeroTouch: High-Performance Sensing for Multi-Touch and Free-Air Interaction</AwardTitle>
    <AwardEffectiveDate>07/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>12/31/2012</AwardExpirationDate>
    <AwardAmount>50000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Rathindra DasGupta</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The proposed technology is a high-performance optoelectronic architecture for multi-finger interaction. As computing resources grow cheap and ubiquitous, the importance of embodying interaction grows, so that people can express themselves to computers with their bodies, in the same rich ways that we communicate with each other. Multitouch and free-air are important new sensory modalities for embodied interaction. Most multi-touch devices on the market are smartphones and tablets using capacitive multi-touch sensing technologies. However, these technologies do not scale well to large displays because of cost issues and technological challenges. They also cannot sense gloved fingers, precluding their use in many application scenarios. In contrast, the proposed effort encompasses a sensing structure that has a competitive cost structure when scaled to large displays, increasing linearly with display size, as opposed to the quadratic increase in costs exhibited by capacitive sensing. The technology does not require actual touching, only interruption of a sensing plane, enabling gloved interaction in emerging markets such as automobiles. It can work outdoors in sunlight, and in hazardous conditions. It is suitable for TV, tele-operation and 3-D scanning. Beyond multi-touch sensing, the technology enables other new modalities, such as pen+touch, haptics+touch, and free-air interaction. The range of potential markets is wide and deep. &lt;br/&gt;&lt;br/&gt;The proposed technology can transform the impact of embodied interaction on society. Potential integration with large scale LCDs can transform users' experiences of media in living rooms and conference rooms alike. The high precision and many-finger tracking of the team?s approach in large formats in social innovation-oriented contexts will enable the development of new forms of Computer Supported Cooperative Work and Play in the information age workplace, education, and home. Integration with automobiles will first enable easier interaction by gloved hands on small control panels; more innovative solutions will use the platform for interaction across the whole dashboard, and using the windshield as a heads-up display. If successfully commercialized, the technology has the potential to open up a new broad array of industrial applications.</AbstractNarration>
    <MinAmdLetterDate>06/26/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>06/26/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1242538</AwardID>
    <Investigator>
      <FirstName>Andruid</FirstName>
      <LastName>Kerne</LastName>
      <EmailAddress>andruid@cs.tamu.edu</EmailAddress>
      <StartDate>06/26/2012</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Texas A&amp;M University Main Campus</Name>
      <CityName>College Station</CityName>
      <ZipCode>778454375</ZipCode>
      <PhoneNumber>9798626777</PhoneNumber>
      <StreetAddress>400 Harvey Mitchell Pkwy South</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
  </Award>
</rootTag>

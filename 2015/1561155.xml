<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Collaborative Research: ArguLex - Applying Automated Analysis to a Learning Progression for Argumentation.</AwardTitle>
    <AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2019</AwardExpirationDate>
    <AwardAmount>117986</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>11040000</Code>
      <Directorate>
        <LongName>Direct For Education and Human Resources</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Undergraduate Education</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>John Cherniavsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Argumentation is fundamental to both science and science education. This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.&lt;br/&gt;&lt;br/&gt;This project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator.</AbstractNarration>
    <MinAmdLetterDate>08/29/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>08/29/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1561155</AwardID>
    <Investigator>
      <FirstName>Mary Anne</FirstName>
      <LastName>Sydlik</LastName>
      <EmailAddress>maryanne.sydlik@wmich.edu</EmailAddress>
      <StartDate>08/29/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Western Michigan University</Name>
      <CityName>Kalamazoo</CityName>
      <ZipCode>490085200</ZipCode>
      <PhoneNumber>2693878298</PhoneNumber>
      <StreetAddress>1903 West Michigan Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Michigan</StateName>
      <StateCode>MI</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7980</Code>
      <Text>Core R&amp;D Programs</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>8244</Code>
      <Text>EHR CL Opportunities (NSF 14-302)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8817</Code>
      <Text>STEM Learning &amp; Learning Environments</Text>
    </ProgramReference>
  </Award>
</rootTag>

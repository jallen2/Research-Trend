<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Medium: Collaborative Research: Novel microLIDAR Design and Sensing Algorithms for Flapping-Wing Micro-Aerial Vehicles</AwardTitle>
    <AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2018</AwardExpirationDate>
    <AwardAmount>194940</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jeffrey Trinkle</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project makes it possible for a tiny robotic bee to sense its distance to any nearby object. Such depth sensing for a small robot insect pushes the limits of sensor and algorithm design in terms size, weight, computing, and power. The key idea is joint design; every part of the robotic insect is optimized together, from wing design and optics to intelligent algorithms and efficient computation. This is possible by inter-disciplinary work across scientists and engineers from diverse backgrounds. The lessons learned through this project can be applied to transform other applications that involve small devices including medical sensors and endoscope imaging, smart homes and the internet of things, agricultural and industrial monitoring systems, and mobile vision for search and rescue.&lt;br/&gt;&lt;br/&gt;Lidar sensing has enabled large robotic cars to navigate complex environments. This proposal introduces designs for "micro-lidar" that can be used on insect-scale aerial robots. Making micro-lidar work on small platforms involves four intertwined research thrusts. The first thrust uses MEMS mirrors and wide-angle optics to sense and modulate the laser pulses. The second thrust is adapting signal processing algorithms to estimate range data at this scale. The third thrust is developing novel perception and navigation algorithms to map the indoor environments using a micro-aerial vehicle. The fourth thrust is to improve robotic insect flight to allow novel manipulations that require knowledge of the surrounding range map. The utility of these sensors will be demonstrated on the robobee for novel maneuvers and building topo-feature maps of indoor environments.</AbstractNarration>
    <MinAmdLetterDate>06/08/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>06/15/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1514306</AwardID>
    <Investigator>
      <FirstName>Robert</FirstName>
      <LastName>Wood</LastName>
      <EmailAddress>rjwood@eecs.harvard.edu</EmailAddress>
      <StartDate>06/08/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Harvard University</Name>
      <CityName>Cambridge</CityName>
      <ZipCode>021385366</ZipCode>
      <PhoneNumber>6174955501</PhoneNumber>
      <StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Massachusetts</StateName>
      <StateCode>MA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7924</Code>
      <Text>MEDIUM PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>

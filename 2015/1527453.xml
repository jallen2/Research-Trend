<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CHS: Small: Supporting Crowdsourced Sensemaking in Big Data with Dynamic Context Slices</AwardTitle>
    <AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2018</AwardExpirationDate>
    <AwardAmount>329392</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>William Bainbridge</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This research will investigate how crowdsourcing and computational techniques can be combined to support the efforts of an individual analyst engaged in a complex sensemaking task, such as identifying a threat to national security or determining the names of people and places in a photograph. Currently, such complex tasks are beyond the capabilities of the most advanced machine learning techniques or crowdsourcing workflows, and even trained experts struggle to perform them. Huge quantities of data are now available online, but making sense of them is challenging because human cognition, while remarkably powerful, is nevertheless a limited resource. Visual analytics tools seek to overcome this limitation by leveraging the complementary strengths of information visualization and data mining, but these tools generally assist with low-level tasks, requiring significant effort on the part of users. Crowdsourcing has emerged as a promising technique for applying human intelligence to problems computers cannot easily solve, but for crowds to assist individuals with complex sensemaking tasks, two significant challenges must be addressed. First, we must understand when crowds versus computation are more useful at each phase in the sensemaking loop. Second, we must overcome the limited time and expertise of most crowd workers to sustain deep, complex lines of inquiry.&lt;br/&gt;&lt;br/&gt;This research addresses both of these challenges through a series of four experiments. First, it will conduct a laboratory study where individuals perform complex sensemaking tasks to understand what types and amounts of context they use to make decisions, and how the sensemaking loop might be decomposed into subtasks. Second, it will conduct a series of experiments comparing crowdsourcing to automated techniques for each of the most promising sensemaking subtasks. Third, it will experiment with different crowd workflows to develop a revised sensemaking loop, optimized for the relative strengths of crowds and computation, and develop a software prototype based on this approach. At the core of the software design is the novel concept of "context slices," an innovative technique for addressing the transience of crowd workers by giving them only the information they need to complete their assigned task, allowing complex investigations to be pursued across multiple workers. The fourth experiment will evaluate this approach by comparing performance with the software to the baselines established in the first study.</AbstractNarration>
    <MinAmdLetterDate>09/09/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>08/01/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1527453</AwardID>
    <Investigator>
      <FirstName>Kurt</FirstName>
      <LastName>Luther</LastName>
      <EmailAddress>kluther@vt.edu</EmailAddress>
      <StartDate>09/09/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Christopher</FirstName>
      <LastName>North</LastName>
      <EmailAddress>north@cs.vt.edu</EmailAddress>
      <StartDate>09/09/2015</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Virginia Polytechnic Institute and State University</Name>
      <CityName>BLACKSBURG</CityName>
      <ZipCode>240610001</ZipCode>
      <PhoneNumber>5402315281</PhoneNumber>
      <StreetAddress>Sponsored Programs 0170</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Virginia</StateName>
      <StateCode>VA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7367</Code>
      <Text>Cyber-Human Systems (CHS)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7367</Code>
      <Text>Cyber-Human Systems</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>

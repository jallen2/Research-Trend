<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>PFI:BIC Human-Centered Smart-Integration of Mobile Imaging and Sensing Tools with Machine Learning for Ubiquitous Quantification of Waterborne and Airborne Nanoparticles</AwardTitle>
    <AwardEffectiveDate>10/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2018</AwardExpirationDate>
    <AwardAmount>1012000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Alexandra Medina-Borja</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This Partnerships for Innovation: Building Innovation Capacity (PFI:BIC) project focuses on the creation of a human-centered smart toolset and service system for on-site and ubiquitous quantification and automated charaterization/classification of nanosize objects. Nanoparticles are being used in more and more commercial and industrial products while their health and environmental implications are still under debate. The toxicity of nanomaterials not only varies among different materials, but is also highly dependent on the dose of exposure. Developing a sensitive method to detect the release and spatio-temporal distribution of nanoparticles in the environment as well as in daily lives is a high priority before their toxicity effects are fully understood via long-term toxicological studies. Despite this urgent need for widespread detection and quantification of nanoparticle distributions, current technologies are lacking appropriate features for ubiquitous and cost-effective mapping and quantification of nanoparticle contamination. This project aims to create a transformative and human-centered toolset for on-site and ubiquitous quantification and automated characterization of nanomaterials found in houses, workplaces and the environment based on the cost-effective integration of computational imaging and mobile sensing techniques with big data based dynamic machine learning algorithms. &lt;br/&gt;&lt;br/&gt;The central challenge in this project is to translate the bulky and expensive laboratory equipment currently used for nanoparticle quantification and characterization to field-portable, easy-to-use, cost-effective, and rapid analysis devices and smart service systems aiming to be massively used by consumers in their daily routines. To solve this challenge, highly sensitive optical imaging systems will be developed based on mass-produced Complementary Metal-Oxide Semiconductor (CMOS) sensor chips embedded in mobile phones with extraordinary signal to noise ratios (SNR) and large fields-of-view for high-throughput machine learning based automated nanoparticle analysis and classification. One approach this will take is to combine computational microscopy with self-assembled nanolenses around nanoparticles that significantly enhance imaging SNR and contrast. The aim of this approach is to enable automated detection and sizing of individual nanoparticles, mono-dispersed samples, and complex poly-dispersed mixtures, where the sample concentrations can span ~5 orders-of-magnitude and particle sizes can range from 40 nm to millimeter-scale, which provide unmatched performance metrics compared to existing nanoparticle sizing approaches. Another approach that will be implemented is the development of highly sensitive multi-modal (e.g. fluorescence plus dark-field) mobile phone based microscopy platforms for distributed nanoparticle imaging and sensing. Furthermore, in terms of big data analysis and machine learning tools, the techniques in this project can adaptively learn "semantic" similarities that can be used for more accurate data classification. These techniques are unlike existing techniques developed so far in the literature. The extant technologies are based only on signal similarities, which do not work well on multi-modality data. The smart and adaptive methods of this project are the first in the literature that come with confidence bounds, that is, they not only have the capability to accurately classify the information, but they also provide guarantees about the accuracy of this classification, which is quite important for self-learning smart service systems. Through these field-portable devices that are integrated with adaptive big data based decision analytics and quantification algorithms, spatio-temporal maps of nanoparticle concentrations and size distributions in various consumer samples will be created for public or personal monitoring (e.g., measurements of waterborne/airborne particles at home, workplace, or airborne particles along a freeway, etc.).&lt;br/&gt;&lt;br/&gt;The broader impacts of this transformative research include (1) The development of these nanoparticle sensing and quantification platforms and smart service systems will extend the boundaries of current optical metrology science, resulting in new advances in the fields of nanophotonics and optical microscopy (2) These devices will also be easy to translate into various biomedical, chemical and material science applications, significantly impacting the use and regulations of nanotechnologies in consumer market and related products. (3) This project would deliver a paradigm-shift by ubiquitous quantification and spatiotemporal mapping/monitoring of nanoparticle contamination and exposure even in non-laboratory settings, assisting in the revelation and better understanding of various cause-effect relationships at the consumer level that have remained unidentified so far due to the limitations of existing nano-imaging, detection and quantification technologies, also providing maps of potential health risks. (4) This project will also establish a complementary educational outreach program based in California.&lt;br/&gt;&lt;br/&gt;The lead institution and primary partners included in this cross-organizational interdisciplinary project are: Lead Academic Institution: University of California, Los Angeles, CA, School of Engineering, Electrical and Bioengineering Departments; Primary Industrial Partner: Holomic LLC (Small Business located in Los Angeles, CA); Other Industrial Partner: Google Inc. (Large Business located in Mountain View, CA).</AbstractNarration>
    <MinAmdLetterDate>08/06/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>04/27/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1533983</AwardID>
    <Investigator>
      <FirstName>Aydogan</FirstName>
      <LastName>Ozcan</LastName>
      <EmailAddress>ozcan@ee.ucla.edu</EmailAddress>
      <StartDate>08/06/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Mihaela</FirstName>
      <LastName>van der Schaar</LastName>
      <EmailAddress>mihaela@ee.ucla.edu</EmailAddress>
      <StartDate>08/06/2015</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Los Angeles</Name>
      <CityName>LOS ANGELES</CityName>
      <ZipCode>900952000</ZipCode>
      <PhoneNumber>3107940102</PhoneNumber>
      <StreetAddress>11000 Kinross Avenue, Suite 211</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1662</Code>
      <Text>PARTNRSHIPS FOR INNOVATION-PFI</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7236</Code>
      <Text>BIOPHOTONICS, IMAGING &amp;SENSING</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>005E</Code>
      <Text>Neuro-photonics</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>116E</Code>
      <Text>RESEARCH EXP FOR UNDERGRADS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>1662</Code>
      <Text>PARTNRSHIPS FOR INNOVATION-PFI</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9251</Code>
      <Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
    </ProgramReference>
  </Award>
</rootTag>

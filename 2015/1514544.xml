<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Medium: Collaborative Research: Understanding Individual-Level Speech Variability: From Novel Articulatory Data to Robust Speaker Recognition</AwardTitle>
    <AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2019</AwardExpirationDate>
    <AwardAmount>585307</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Speech is a unique human capability. The vocal tract is the universal human instrument played with great dexterity and skill in the production of speech to convey rich linguistic and paralinguistic information. The project will enable fundamental understanding of how individuals differ in their speech articulation due to differences in shape and size of their physical vocal instrument. Knowledge of how people differ in their speech production can help create improved automatic speaker recognition, technologies important for national security. The project can inform design of technologies for robust speech-based access for all members of the population, including children, the elderly, and non-native speakers of a language. Results from the project can also assist in better understanding and treating disorders (e.g., cleft lip/palate), illness (e.g., head and neck cancer, apnea) or injury where human speech articulation is affected. The novel imaging data from 200 individuals, and associated tools, annotations and interpretations created by the interdisciplinary team will be shared broadly with the scientific community. The project will provide a unique research training opportunity for students in integrated speech science and technology. &lt;br/&gt;&lt;br/&gt;The overarching goal of this project is to advance scientific understanding of how vocal tract morphology and speech articulation interact and explain the variant and invariant aspects of speech signal properties across talkers. Of particular scientific interest is the nature of articulatory strategies adopted by individuals in the presence of structural differences across them to achieve phonetic equivalence. Equally of interest are what aspects of, and how, vocal tract morphological differences are reflected in the acoustic speech signal, and if those differences can be estimated from speech acoustics. A crucial part of this goal is to create forward and inverse computational models that relate vocal tract details to speech acoustics toward shedding light on individual speaker differences and informing design of robust speaker recognition technologies. This project goes beyond state-of-the-art methods by focusing on direct investigation of the dynamic human vocal tract using novel imaging techniques and computational modeling to illuminate inter-speaker variability in vocal tract structure, as well as the strategies by which linguistic articulation is implemented. Using novel Magnetic Resonance Imaging with superior spatial resolution of the entire moving vocal tract that we helped develop (dynamic realtime 2D with excellent temporal resolution and accelerated volumetric 3D), the project will gather and quantify spatio-temporal details of speech production from 160 native American English covering the major dialectal regions of North America and 40 non-native speakers. The experimental, theoretical, and methodological approaches investigating the interplay between structure (shape and size) and function (dynamics of vocal-tract shaping and its acoustic consequences) can lead to new theoretical advances with improved phonetic characterizations of linguistic units that are general across speakers. It also offers the ability to explain individual specific speech patterns that can improve both understanding the scientific underpinning and creating robust automatic speaker recognition technology, enabling to determine not only that two talkers are different by the adoption of novel speaker dependent features, but also how and why they differ, by analyzing biologically-inspired details of structure and articulation.</AbstractNarration>
    <MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>06/17/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1514544</AwardID>
    <Investigator>
      <FirstName>Shrikanth</FirstName>
      <LastName>Narayanan</LastName>
      <EmailAddress>shri@sipi.usc.edu</EmailAddress>
      <StartDate>08/31/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Krishna</FirstName>
      <LastName>Nayak</LastName>
      <EmailAddress>knayak@usc.edu</EmailAddress>
      <StartDate>08/31/2015</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Southern California</Name>
      <CityName>Los Angeles</CityName>
      <ZipCode>900890001</ZipCode>
      <PhoneNumber>2137407762</PhoneNumber>
      <StreetAddress>University Park</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7924</Code>
      <Text>MEDIUM PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>

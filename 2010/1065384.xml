<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>G&amp;V: Medium: Collaborative Research: Contact-Based Human Motion Acquisition and Synthesis</AwardTitle>
    <AwardEffectiveDate>04/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>03/31/2017</AwardExpirationDate>
    <AwardAmount>318570</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>To date, motion capture technologies suffer from three major limitations. First, the hardware devices are restrictive, cumbersome, and expensive. Second, most techniques only record the kinematic information of the movement, rather than underlying dynamic properties or control mechanisms. Third, the current technique fails to capture the interaction between the subject and the environment. Without the information of contacts, reconstructing motion that consists of complex contact phenomena is nearly impossible. This project develops a new motion acquisition and reconstruction technique that solves all three problems aforementioned. The new technique combines the force sensors and a single video camera to reconstruct full-body poses, joint torques, and contact forces in an unconstrained setting. In contrast to expensive lab equipment, the proposed system consists of a pair of low-cost, non-intrusive force-sensing shoes and a single consumer-level video camera that can be used to acquire motions difficult to capture in the lab. This acquisition technology enables new design of motion controllers by leveraging a large amount of real-world contact data. The research also develops new data representations and novel algorithms for intelligent and efficient motion planning and evaluates the developed motion controllers by simulating a human figure performing challenging balanced activities in a novel and unpredicted environment. &lt;br/&gt;&lt;br/&gt;The project is tightly integrated with education components in both Georgia Tech and Texas A&amp;M. The research of this project lends itself well to solve important real-world problems for computer graphics. The results from this project would impact research in video gaming, sports training, remote health care, biped robots, and virtual characters, etc.</AbstractNarration>
    <MinAmdLetterDate>03/25/2011</MinAmdLetterDate>
    <MaxAmdLetterDate>06/03/2013</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1065384</AwardID>
    <Investigator>
      <FirstName>Jinxiang</FirstName>
      <LastName>Chai</LastName>
      <EmailAddress>jchai@cs.tamu.edu</EmailAddress>
      <StartDate>03/25/2011</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Texas A&amp;M Engineering Experiment Station</Name>
      <CityName>College Station</CityName>
      <ZipCode>778454645</ZipCode>
      <PhoneNumber>9798477635</PhoneNumber>
      <StreetAddress>TEES State Headquarters Bldg.</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7453</Code>
      <Text>GRAPHICS &amp; VISUALIZATION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7453</Code>
      <Text>GRAPHICS &amp; VISUALIZATION</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7924</Code>
      <Text>MEDIUM PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Neurodynamics of Tonality</AwardTitle>
    <AwardEffectiveDate>08/15/2010</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2014</AwardExpirationDate>
    <AwardAmount>403129</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Anne Cleary</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Music is a high-level cognitive capacity, a form of communication that relies on highly structured temporal sequences comparable in complexity to language. Music is found among all human cultures, and musical "languages" vary among cultures and depend upon learning. For example, European melodies use different kinds of note combinations than Indian melodies, making it difficult for Westerners to understand Indian music, and vice versa. Unlike language, however, music rarely refers to the external world. It consists of self-contained patterns of sound, aspects of which are found universally among musical cultures. Therefore, while an understanding of the brain processes underlying language is still a distant goal, discovering the general principles of neural dynamics that underlie music may now be possible. Tonality refers to the stability relationships that are perceived among notes in a musical language. Although there are different kinds of tonality, tonality itself is a universal feature of music, found in virtually every musical language. The hypothesis of this research is that neural oscillation underlies tonal cognition and perception. Neural oscillation is periodic neural activity that, in the auditory system, becomes time-locked to incoming sounds. Neural oscillations can be complex, but there are now powerful mathematical tools for analyzing them. Mathematical analyses of time-locking auditory dynamics suggest constraints on what sorts of tonal relationships should be possible. They predict that fundamental principles of neural dynamics combined with fundamental principles of neural plasticity constrain what musical languages can be learned.&lt;br/&gt;&lt;br/&gt;To make detailed predictions, a sophisticated computer model of the auditory system will be built, based on the organization of the auditory system and general neurodynamic principles. Two simulations will be trained through passive exposure to European and North Indian melodies. These two are chosen because they represent two very different musical languages that are each relatively well-studied. The computer model will be used to predict neurophysiological and perceptual observations about music perception that have been collected over the past thirty-five years or so. Success of this model would imply the existence of a musical universal grammar. Universals predicted by intrinsic neurodynamics would provide a direct link to neurophysiology, and explain how brain changes during learning can establish different musical languages. This could lead to fundamental paradigm shifts in music theory, music cognition and related fields. The success of this model would be equally influential in cognitive neuroscience. It would imply that high-level cognition and perception can arise from the interaction of acoustic signals with the physics of the auditory system. No neurodynamic approach has ever successfully captured such a high-level cognitive capacity. Researchers are currently struggling with the question of how to reconcile cognitive theories with neurodynamic principles and observations, and success in the musical domain could lead to new insights. This research will elucidate fundamental mechanisms of hearing and communication, and holds significant promise for understanding auditory system development. Identification of innate constraints shaping human communication behavior may have further implications for language learning. This research has implications for understanding a wide range of hearing and communication disorders. It has potential applicability to improving the design of neural prostheses, enhancing the perception of music and other sounds in cochlear implant patients.</AbstractNarration>
    <MinAmdLetterDate>08/15/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>06/25/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1027761</AwardID>
    <Investigator>
      <FirstName>Edward</FirstName>
      <LastName>Large</LastName>
      <EmailAddress>edward.large@uconn.edu</EmailAddress>
      <StartDate>08/15/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Florida Atlantic University</Name>
      <CityName>BOCA RATON</CityName>
      <ZipCode>334316424</ZipCode>
      <PhoneNumber>5612970777</PhoneNumber>
      <StreetAddress>777 GLADES RD</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Florida</StateName>
      <StateCode>FL</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Improving Computerized Adaptive Testing In the United States</AwardTitle>
    <AwardEffectiveDate>03/15/2003</AwardEffectiveDate>
    <AwardExpirationDate>03/31/2006</AwardExpirationDate>
    <AwardAmount>180000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04050300</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Divn Of Social and Economic Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Cheryl L. Eavey</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>While the implementation of computerized adaptive testing (CAT) has many advantages, many issues related to CATs are not well understood. This project will study three specific issues in development and implementation of CAT: (1) compatibility between CAT and "paper and pencil" (P&amp;P) tests, (2) test security and item pool usage, and (3) how to calibrate test items in large quantities efficiently and economically. With respect to compatibility between CAT and "paper and pencil" (P&amp;P) tests, it has been widely reported that some students get much lower scores than they would if an alternative P&amp;P version were given. However, examinees currently required to take Graduate Record Examination (GRE) in the United States, for instance, are not given a choice between the standard P&amp;P version of the tests and the CAT versions. Without effective remedial measures, the credibility of CAT could be significantly undermined. This project proposes to modify the statistical procedure used for CAT item selection by incorporating some advanced analytic techniques. It is expected that the analytic and simulation results will show that weighting likelihood score may alleviate the problem of underestimation. With respect to test security and item pool usage, in current operational CATs, computers tend to select certain types of items too frequently, making item exposure rates quite uneven. This project will show that test security and the underestimation problem discussed in the research on CAT and P&amp;P tests are closely related. It is also expected that the project will show that the alpha-stratified approach proposed by Chang and Ying in 1999 tends to improve both the underestimation and test security. With respect to calibrating test items in large quantities efficiently and economically, administration of CATs requires very large item pools. Fortunately, CAT provides great potential to large-scale calibration during on-line testing. This project will explore the development of on-line calibration in CAT.&lt;br/&gt;&lt;br/&gt;CAT has become a popular mode of educational assessment in the United States. Examples of large scale CATs include the Graduate Record Examination (GRE), the Graduate Management Admission Test (GMAT), the National Council of State Boards of Nursing, and the Armed Services Vocational Aptitude Battery (ASVAB). Findings from this research project may speed up the process of improvement over current item selection algorithms. Because many CATs are high-stakes examinations, improving their test reliabilities will greatly benefit society.</AbstractNarration>
    <MinAmdLetterDate>03/18/2003</MinAmdLetterDate>
    <MaxAmdLetterDate>03/18/2003</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0241020</AwardID>
    <Investigator>
      <FirstName>Hua-Hua</FirstName>
      <LastName>Chang</LastName>
      <EmailAddress>hhchang@illinois.edu</EmailAddress>
      <StartDate>03/18/2003</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Texas at Austin</Name>
      <CityName>Austin</CityName>
      <ZipCode>787121532</ZipCode>
      <PhoneNumber>5124716424</PhoneNumber>
      <StreetAddress>101 E. 27th Street, Suite 5.300</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1333</Code>
      <Text>METHOD, MEASURE &amp; STATS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>

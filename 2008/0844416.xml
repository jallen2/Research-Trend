<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Image Super-Resolution Using Trillions of Examples</AwardTitle>
    <AwardEffectiveDate>02/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2014</AwardExpirationDate>
    <AwardAmount>219408</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Sylvia J. Spengler</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Many important image processing tasks involve solving some type of inverse problem. Examples include: eliminating noise, compensating for low quality optical systems, enhancing a black-and-white image with plausible colors and improving the resolution of an image. All are ill-conditioned and so their solutions must incorporate assumptions about natural images. Although researchers have made astounding progress on these problems in the last fifty years, a key limiting property of current techniques is that they analyze their input more or less in isolation.&lt;br/&gt;&lt;br/&gt;This research will ask the question of whether an image can be improved by evaluating trillions of image patches constructed from millions of on-line images and using the set of relevant patches to improve the quality of the original. This work will focus on one particular inverse problem: super-resolution, or increasing the resolution of an image to reveal missing details. The IBM/Google compute cluster in conjunction with the MapReduce programming framework will be instrumental in developing and evaluating these data-intensive algorithms. This research will investigate scaling existing example-based techniques to use a massive training database and develop entirely new techniques that better capitalize on this amount of data by incorporating higher-level patterns in images such as scene categories and object boundaries.</AbstractNarration>
    <MinAmdLetterDate>02/02/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>11/19/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0844416</AwardID>
    <Investigator>
      <FirstName>Jason</FirstName>
      <LastName>Lawrence</LastName>
      <EmailAddress>jdl@cs.virginia.edu</EmailAddress>
      <StartDate>02/02/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Virginia Main Campus</Name>
      <CityName>CHARLOTTESVILLE</CityName>
      <ZipCode>229044195</ZipCode>
      <PhoneNumber>4349244270</PhoneNumber>
      <StreetAddress>P.O. BOX 400195</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Virginia</StateName>
      <StateCode>VA</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0116000</Code>
      <Name>Human Subjects</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>7782</Code>
      <Text>CLUSTER EXPLORATORY (CLuE)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7782</Code>
      <Text>CLUSTER EXPLORATORY (CLuE)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9216</Code>
      <Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>HPCC</Code>
      <Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
    </ProgramReference>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CSR: Small: Bridging Efficiency and Low Latency in Warehouse-scale Computing</AwardTitle>
    <AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2017</AwardExpirationDate>
    <AwardAmount>466783</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05050000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Computer and Network Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>M. Mimi McClure</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Computing is now an essential tool and a catalyst for innovation for all aspects of human endeavor, including healthcare, education, science, commerce, government, and entertainment. An increasing amount of computing is performed on private and public clouds, primarily due to the cost and scalability benefits for both the end-users and operators of the warehouse-scale systems that host clouds. We have come to expect that these systems provide millions of users with instantaneous, personalized, and contextual access to petabytes of data. The goal of this project is to improve the capabilities and efficiency of warehouse-scale systems. Specifically, we aim to reconcile the presumed incompatibility between low-latency processing at massive scales and efficiency in terms of energy consumption and resource usage. We aim to improve energy and resource efficiency in warehouse-scale systems by factors of 2x-5x while allowing for low-latency processing at massive scales. Equally important, we aim to improve our understanding of the tradeoffs between scalability, low latency, and energy or resource efficiency in modern computing systems.&lt;br/&gt;&lt;br/&gt;The project focuses on on-line, data-intensive workloads, such as search, social networking, real-time analytics, and machine learning analysis, that occupy thousands of servers in warehouse-scale systems and pose significant scaling challenges. Their strict latency constraints, large state requirements, and high communication fan-out makes it difficult to apply known techniques for power reduction and resource sharing across applications. Hence, they typically consume a significant percentage of peak power and use non-shared servers even during the frequent periods of medium or low user traffic. To a large extent, low latency and high efficiency are considered incompatible for these workloads. To bridge this gap, the project uses a cross-layer approach that monitors end-to-end workload performance and quality-of-service to guide system-wide power management and resource management. The first step is to develop a power management system that improves the energy proportionality of warehouse-scale systems during periods of low or medium load without compromising latency guarantees. The second step is to develop a system-wide resource management system that allows aggressive server sharing between latency-critical workloads and other workloads during periods of low or medium load without compromising latency guarantees. The third step is to design operating system policies for performance isolation between co-located workloads within a server. The final step is to use the insights from the previous steps to evaluate the efficacy of existing and proposed server architectures with respect to energy and resource efficiency for on-line, data-intensive workloads.</AbstractNarration>
    <MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1422088</AwardID>
    <Investigator>
      <FirstName>Christoforos</FirstName>
      <LastName>Kozyrakis</LastName>
      <EmailAddress>kozyraki@stanford.edu</EmailAddress>
      <StartDate>08/04/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Stanford University</Name>
      <CityName>Palo Alto</CityName>
      <ZipCode>943041212</ZipCode>
      <PhoneNumber>6507232300</PhoneNumber>
      <StreetAddress>3160 Porter Drive</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7354</Code>
      <Text>COMPUTER SYSTEMS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>

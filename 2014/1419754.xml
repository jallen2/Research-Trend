<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Inference After Predictor Selection</AwardTitle>
    <AwardEffectiveDate>09/01/2013</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2017</AwardExpirationDate>
    <AwardAmount>139999</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040000</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Gabor J. Szekely</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>There are three goals for this project. The first goal is to develop data-driven assessments of the complexity of data generators and data-driven assessments of the complexity of the predictive techniques to be used for a data generator and then relate them to each other. It is expected that a complexity matching principle between data generators and their predictors will be established. The motivation is to speed the search for predictors that have low generalization error. The second goal is to develop techniques to derive modeling information from good predictors. The motivation is to be able to make statements about the data generator beyond numerical prediction. The third goal is to use these techniques on a complex data set for which a predictive approach is essential because the extreme complexity of the data means it defies conventional modeling. The motivation is to verify that the complexity based techniques give reliable inferences for an important question such as `which of those who have suffered a traumatic event are likely to get post- traumatic stress disorder'.&lt;br/&gt;&lt;br/&gt;The motivation for the overall project is to find ways to get information out of data that is so complex conventional techniques are ineffective. Such data is becoming increasingly common as the number of data types increases and as data bases become more comprehensive. The problem with conventional techniques seems to be that they assume a model that means something physically before there is a strong enough basis even to propose one. The approach here is significant because it is overtly predictive: Instead of proposing models, one can propose predictors that are easier to test and then study the predictors to make statements about whatever it was that generated the data. This reverses the usual approach in which one models first and then predicts.</AbstractNarration>
    <MinAmdLetterDate>01/27/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>01/27/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1419754</AwardID>
    <Investigator>
      <FirstName>BERTRAND</FirstName>
      <LastName>CLARKE</LastName>
      <EmailAddress>bclarke3@unl.edu</EmailAddress>
      <StartDate>01/27/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Nebraska-Lincoln</Name>
      <CityName>Lincoln</CityName>
      <ZipCode>685031435</ZipCode>
      <PhoneNumber>4024723171</PhoneNumber>
      <StreetAddress>151 Prem S. Paul Research Center</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Nebraska</StateName>
      <StateCode>NE</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1269</Code>
      <Text>STATISTICS</Text>
    </ProgramElement>
  </Award>
</rootTag>

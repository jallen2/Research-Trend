<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Collaborative Research: Cognitive models of the acquisition of vowels in context</AwardTitle>
    <AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2018</AwardExpirationDate>
    <AwardAmount>240000</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Models of infant language acquisition allow researchers to test hypotheses about how infants learn their native language from speech. This project focuses on how infants determine which sounds are meaningful in their language and concentrates on co-articulation, a process in which sounds are produced differently depending on qualities of neighboring sounds. Co-articulation is closely tied to the historical creation, change, and loss of sound patterns in language. As a result, the models developed in this project further our understanding of both infant language acquisition and historical language change. Knowing how different aspects of the learning process interact can also help pinpoint deficits that might underlie developmental language disorders. Finally, building models of language acquisition that can work on speech data can potentially help create speech recognition technology that adapts robustly to novel languages, accents, and domains of discourse in the same way that human learners do.&lt;br/&gt;&lt;br/&gt;A series of unsupervised phonetic learning models are created to examine how learners take into account co-articulation. Whereas previous models of phonetic learning have categorized each sound independently, these models begin with a set of constraints that capture possible dependences between sounds, formalized using a Markov random field. They then learn from the data which constraints characterize a particular language. Recordings of child-directed speech from the CHILDES database are annotated through forced alignment to serve as test corpora for comparing the newly developed models with previous models. The project yields a rigorous evaluation of where existing models fall short, a new framework for accounting for phonetic variability, and corpora to support the development and evaluation of future phonetic learning models.</AbstractNarration>
    <MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>06/15/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1421695</AwardID>
    <Investigator>
      <FirstName>Naomi</FirstName>
      <LastName>Feldman</LastName>
      <EmailAddress>nhf@umd.edu</EmailAddress>
      <StartDate>08/18/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Maryland College Park</Name>
      <CityName>COLLEGE PARK</CityName>
      <ZipCode>207425141</ZipCode>
      <PhoneNumber>3014056269</PhoneNumber>
      <StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Maryland</StateName>
      <StateCode>MD</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7252</Code>
      <Text>Perception, Action and Cognition</Text>
    </ProgramReference>
  </Award>
</rootTag>

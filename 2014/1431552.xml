<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>The Effectiveness of Intelligent Virtual Humans in Facilitating Self-Regulated Learning in STEM with MetaTutor</AwardTitle>
    <AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2017</AwardExpirationDate>
    <AwardAmount>1365603</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>11090000</Code>
      <Directorate>
        <LongName>Direct For Education and Human Resources</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Research On Learning</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Gregg E. Solomon</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The investigators will research how characteristics of intelligent virtual humans (IVHs) support the ability of students to reflect on and, therefore, improve their learning in undergraduate biology. To date, research has shown mixed effectiveness when human avatars are used in learning technologies. To remedy that, the researchers will first study how expert human tutors use verbal and facial cues in reacting to students' cognitive, affective, metacognitive, and motivational (CAMM) processes. Then, they will use these data to build an enhanced intelligent virtual human tutor (by altering software called "MetaTutor"). The project will advance the field's ability to build more effective intelligent tutors and advance understanding of self-regulated learning. &lt;br/&gt;&lt;br/&gt;The researchers propose to experimentally study the effectiveness of the enhanced IVHs on learners' self-regulatory processes and other learning outcomes. Data will be collected on both a natural face and a natural face that has been morphed and presented as a virtual human. The facial and verbal expressions are meant to provide learners with an additional information source they can use to monitor and regulate their ongoing self-regulatory processes, including making accurate emotional appraisals. In addition to the facial data, the researchers will collect self-report data, trace data using a variety of sensors, learning outcomes (e.g., pretest and posttest), and knowledge construction activities (e.g., summaries of content, notes, quizzes). Finally, the project will be disseminated in the form of journal publications, conference presentations, and an enhanced version of MetaTutor.</AbstractNarration>
    <MinAmdLetterDate>08/06/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/06/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1431552</AwardID>
    <Investigator>
      <FirstName>James</FirstName>
      <LastName>Lester</LastName>
      <EmailAddress>lester@csc.ncsu.edu</EmailAddress>
      <StartDate>08/06/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Roger</FirstName>
      <LastName>Azevedo</LastName>
      <EmailAddress>razeved@ncsu.edu</EmailAddress>
      <StartDate>08/06/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>North Carolina State University</Name>
      <CityName>RALEIGH</CityName>
      <ZipCode>276957514</ZipCode>
      <PhoneNumber>9195152444</PhoneNumber>
      <StreetAddress>CAMPUS BOX 7514</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>North Carolina</StateName>
      <StateCode>NC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7980</Code>
      <Text>Core R&amp;D Programs</Text>
    </ProgramElement>
  </Award>
</rootTag>

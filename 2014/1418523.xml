<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SCH: INT: Collaborative Research: Learning and Sensory-based Modeling for Adaptive Web-Empowerment Trauma Treatment</AwardTitle>
    <AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2018</AwardExpirationDate>
    <AwardAmount>251757</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Mental trauma following disasters, military service, accidents, domestic violence and other traumatic events is a health issue costing multiple billion of dollars per year. Beyond its direct costs, there are indirect costs including a 45-150% greater use of medical and psychiatric care. While web-based support systems have been developed these are effectively a "one-size-fits all" approach lacking the personalization of regular treatment and the engagement and effectiveness associated with a tailored regimen. This project brings together a multi-disciplinary team of leading researchers in trauma treatment, facial analysis, computer vision and machine learning to develop a scalable, adaptive person-centered approach that uses vision and sensing to improve web-based trauma treatment. In particular, the effort measures specific personalized variables during treatment and then uses a model to adapt treatment to individuals in need. &lt;br/&gt;&lt;br/&gt;The core treatment design builds on well-established social-cognitive theory, where self-efficacy and physiological response are critical elements of recovery. The project measures these as well as engagement that is critical in self-directed web-based treatment. The modeling requires advances in computer vision and facial analysis to develop individualized models that can be computed with just a standard laptop. This project is the first effort to approximate changes in self-efficacy from sensory data. The effort uses and advances machine learning and domain adaption to support this approximation as well as to support the rapid personalization of models. Building a smart system that empowers individuals by combining sensing and learning to improve web-based treatment offers a transformative approach to this national health need for cost-effective evidence-based treatment of trauma.</AbstractNarration>
    <MinAmdLetterDate>08/13/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>03/03/2017</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1418523</AwardID>
    <Investigator>
      <FirstName>Simon</FirstName>
      <LastName>Lucey</LastName>
      <EmailAddress>slucey@cs.cmu.edu</EmailAddress>
      <StartDate>03/03/2017</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Fernando</FirstName>
      <LastName>De la Torre</LastName>
      <EmailAddress>ftorre@cs.cmu.edu</EmailAddress>
      <StartDate>08/13/2014</StartDate>
      <EndDate>03/03/2017</EndDate>
      <RoleCode>Former Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Carnegie-Mellon University</Name>
      <CityName>PITTSBURGH</CityName>
      <ZipCode>152133815</ZipCode>
      <PhoneNumber>4122689527</PhoneNumber>
      <StreetAddress>5000 Forbes Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>8018</Code>
      <Text>Smart and Connected Health</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>8018</Code>
      <Text>Smart and Connected Health</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8062</Code>
      <Text>SCH Type II: INT</Text>
    </ProgramReference>
  </Award>
</rootTag>

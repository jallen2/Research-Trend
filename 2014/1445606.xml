<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Bridges: From Communities and Data to Workflows and Insight</AwardTitle>
    <AwardEffectiveDate>12/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>11/30/2019</AwardExpirationDate>
    <AwardAmount>17209167</AwardAmount>
    <AwardInstrument>
      <Value>Cooperative Agreement</Value>
    </AwardInstrument>
    <Organization>
      <Code>05090000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Robert Chadduck</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>1. Abstract: Nontechnical Description&lt;br/&gt;&lt;br/&gt;The Pittsburgh Supercomputing Center (PSC) will provide an innovative and groundbreaking high-performance computing (HPC) and data-analytic system, Bridges, which will integrate advanced memory technologies to empower new communities, bring desktop convenience to HPC, connect to campuses, and intuitively express data-intensive workflows.&lt;br/&gt;&lt;br/&gt;To meet the requirements of nontraditional HPC communities, Bridges will emphasize memory, usability, Title and effective data management, leveraging innovative new technologies to transparently benefit applications and lower the barrier to entry.&lt;br/&gt;Three tiers of processing nodes with shared memory ranging from 128GB to 12TB will address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. Flexible node allocation will enable interactive use for debugging, analytics, and visualization. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. &lt;br/&gt;&lt;br/&gt;Bridges will host a variety of popular gateways and portals through which users will easily be able to access its resources. Its many nodes will allow long-running jobs, flexible access to interactive use (for example, for debugging, analytics, and visualization, and access to nodes with more memory. Bridges will host a broad spectrum of application software, and its familiar operating system and programming environment will support high-productivity programming languages and development tools.&lt;br/&gt;&lt;br/&gt;Bridges will address data management at all levels. Its shared Project File System, connected to processing nodes by a very capable, appropriately scaled fabric, will provide high-bandwidth, low-latency access to large datasets. Storage on each node will provide local filesystem space that is frequently requested by users and will prevent congestion to the shared filesystem. A set of nodes will be optimized for and dedicated to running databases to support gateways, workflows, and applications. Dedicated web server nodes will enable distributed workflows.&lt;br/&gt;&lt;br/&gt;Bridges will introduce powerful new CPUs and GPUs, and a new interconnection fabric to connect them. These new technologies will be supported by extremely broad set of applications, libraries, and easy-to-use programming languages and tools.&lt;br/&gt;Bridges will interoperate with and complement other NSF Advanced Cyberinfrastructure resources and large scientific instruments such as telescopes and high-throughput genome sequencers, and it will provide convenient bridging to campuses.&lt;br/&gt;&lt;br/&gt;Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools.&lt;br/&gt;&lt;br/&gt;2. Abstract: Technical Description&lt;br/&gt;&lt;br/&gt;The Pittsburgh Supercomputing Center will substantially increase the scientific output of a large community of scientific and engineering researchers who have not traditionally used high-performance computing (HPC) resources. This will be accomplished by the acquisition, deployment, and management of Bridges, a HPC system designed for extreme flexibility, functionality, and usability. Bridges will be supported by operations, user service, and networking staff attuned to the needs of these new user communities, and it will offer a wide range of software appropriate for nontraditional HPC research communities. Users will be able to access Bridges through a variety of popular gateways and portals, and the system will provide development tools for gateway building.&lt;br/&gt;&lt;br/&gt;Innovative capabilities to be introduced by Bridges are:&lt;br/&gt;&lt;br/&gt; 1. Three tiers of processing nodes will offer 128GB, 3TB, and 12TB of hardware-supported, coherent shared memory per node to address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. The 12TB nodes, featuring a proprietary, high-bandwidth internal communication fabric, will be particularly valuable for genome sequence assembly, graph analytics, and machine learning. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. Flexible node allocation will enable interactive use for debugging, analytics, and visualization.&lt;br/&gt;&lt;br/&gt; 2. Bridges will provide integrated, full-time relational and NoSQL databases to support metadata, data management and efficient organization, gateways, and workflows. Database nodes will include SSDs for high IOPs and will be allocated through an extension to the XRAC process. Dedicated web server nodes with high-bandwidth connections to the national cyberinfrastructure will enable distributed workflows. The system topology will provide balanced bandwidth for nontraditional HPC workloads and data-intensive computing.&lt;br/&gt;&lt;br/&gt; 3. Bridges will introduce powerful new CPUs (Intel Haswell and Broadwell), GPUs (NVIDIA GK210 and GP100), the innovative, high-performance Intel Omni Scale Fabric to support increasingly productive development of advanced applications, supported by an extremely broad set of applications, libraries, and easy-to-use programming languages and tools such as OpenACC, parallel MATLAB, Python, and R.&lt;br/&gt;&lt;br/&gt; 4. A shared Project File System (PFS) will provide high-bandwidth, low-latency access to large datasets. Each node will also provide distributed, high-performance storage to support many emerging applications, intermediate and temporary storage, and reduce congestion on the shared PFS.&lt;br/&gt;&lt;br/&gt;Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools.</AbstractNarration>
    <MinAmdLetterDate>11/20/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>03/10/2017</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1445606</AwardID>
    <Investigator>
      <FirstName>Michael</FirstName>
      <LastName>Levine</LastName>
      <EmailAddress>levine@psc.edu</EmailAddress>
      <StartDate>11/20/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Ralph</FirstName>
      <LastName>Roskies</LastName>
      <EmailAddress>roskies@psc.edu</EmailAddress>
      <StartDate>11/20/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Nicholas</FirstName>
      <LastName>Nystrom</LastName>
      <EmailAddress>nystrom@psc.edu</EmailAddress>
      <StartDate>11/20/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>JRay</FirstName>
      <LastName>Scott</LastName>
      <EmailAddress>scott@psc.edu</EmailAddress>
      <StartDate>11/20/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Carnegie-Mellon University</Name>
      <CityName>PITTSBURGH</CityName>
      <ZipCode>152133815</ZipCode>
      <PhoneNumber>4122689527</PhoneNumber>
      <StreetAddress>5000 Forbes Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7619</Code>
      <Text>EQUIPMENT ACQUISITIONS</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7726</Code>
      <Text>DATANET</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7433</Code>
      <Text>CyberInfra Frmwrk 21st (CIF21)</Text>
    </ProgramReference>
  </Award>
</rootTag>

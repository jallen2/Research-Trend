<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Exploring neurobiological strategies of visual scene analysis using oscillations in recurrent neural circuitry</AwardTitle>
    <AwardEffectiveDate>10/01/2007</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2011</AwardExpirationDate>
    <AwardAmount>402133</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Recurrent connections and intrinsic rhythmic neural activity are ubiquitous in biological visual pathways, where they first appear in the retina. Although the functional role of these properties of the retinal circuit is not fully resolved, recent work suggests that they might help convey information about visual context or even the gist of a scene. Further, new experimental work shows that the oscillatory patterns of activity generated by retinal networks are relayed from the thalamus to the cortex. As yet, however, retinal oscillations are usually ignored in systems of computer vision, even those based on neural principles. If artificial systems matched human performance in visual perception, this lack of attention to the biological circuitry might not be worthy of note. But this is not the case. Humans do a far better job than computers in routine tasks like analyzing cluttered scenes or recognizing objects in noisy backgrounds or under different lighting conditions. Thus this project aims to develop biologically inspired models that include oscillating networks to improve scene analysis in artificial vision.&lt;br/&gt;&lt;br/&gt;The new models will incorporate local and distributed connections in the retinal circuit and also take advantage of the scheme of efficient sparse coding, a powerful new concept for understanding sensory processing. By taking both local and spatially extensive circuits into account, the expectation is that the model will be able to encode two complementary types of information about the stimulus. Past work has shown that changes in spike rate with respect to the stimulus encode information about local features. This type of rate (or stimulus-locked) coding can be modeled by small scale circuits. The new models will include ongoing oscillatory activity that is generated internally by large recurrent networks but is also modulated by sensory input. Further, visually evoked changes in the temporal structure of these intrinsic oscillations occur at finer time scales than visually evoked changes in rate. Thus, in principle, visual information could be encoded by spike timing with respect to intrinsic rhythms. Moreover, since the retinal oscillations are generated by distributed networks, it is likely that they provide information about global feature of the stimulus. Thus, if successful, the new models will be able to capture, at once, information about local detail and the gist of scene.&lt;br/&gt;&lt;br/&gt;Numerous applications would benefit from a deeper understanding of how information is encoded in the early visual system. For example, the models that will result from the research proposed here have value for the development of visual prosthetics as well as for technical applications that involve image-processing from new methods for image compression adapted to sensory perception to the problem of automated object segmentation, scene analysis and recognition.</AbstractNarration>
    <MinAmdLetterDate>09/19/2007</MinAmdLetterDate>
    <MaxAmdLetterDate>09/19/2007</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0713657</AwardID>
    <Investigator>
      <FirstName>Friedrich</FirstName>
      <LastName>Sommer</LastName>
      <EmailAddress>fsommer@berkeley.edu</EmailAddress>
      <StartDate>09/19/2007</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Berkeley</Name>
      <CityName>BERKELEY</CityName>
      <ZipCode>947045940</ZipCode>
      <PhoneNumber>5106428109</PhoneNumber>
      <StreetAddress>Sponsored Projects Office</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
  </Award>
</rootTag>

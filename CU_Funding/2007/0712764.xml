<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Cross-Task Learning for Natural Language Processing</AwardTitle>
    <AwardEffectiveDate>08/15/2007</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2011</AwardExpirationDate>
    <AwardAmount>377067</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project considers the problem of simultaneously solving multiple&lt;br/&gt;component-level natural language processing problems. Such&lt;br/&gt;component-level tasks are necessary as building blocks for large-scale&lt;br/&gt;applications (eg., automatic document summarization, machine&lt;br/&gt;translation, etc.), but are typically solved independently. These&lt;br/&gt;independent solutions ignore the natural connections that relate the&lt;br/&gt;output of one problem to the output of the other. This research&lt;br/&gt;explores the ability to exploit such output correspondences to aid&lt;br/&gt;machine learning algorithms, termed "Cross-Task Learning." These&lt;br/&gt;output correspondences provide strong prior information about the&lt;br/&gt;relationship between the desired outputs of multiple problems. This&lt;br/&gt;prior knowledge can potentially serve to improve task-level&lt;br/&gt;performance, even when large amounts of training data are unavailable.&lt;br/&gt;The research exploits such prior knowledge using a k-best methodology&lt;br/&gt;so as to maximize the applicability of these techniques. It also&lt;br/&gt;develops new techniques for semi-supervised learning based on the idea&lt;br/&gt;of output correspondences in order to capitalize on the vast amounts&lt;br/&gt;of unannotated data that are available. In addition, the proposed techniques&lt;br/&gt;are analyzed in the context of computational learning theory. The outcome &lt;br/&gt;will be a set of techniques for learning across multiple natural language processing&lt;br/&gt;tasks. This technology will be empirically evaluated in the context&lt;br/&gt;of low-level tasks such as shallow parsing and named entity&lt;br/&gt;recognition, as well as the high-level tasks of discourse analysis and&lt;br/&gt;automatic document summarization.</AbstractNarration>
    <MinAmdLetterDate>08/16/2007</MinAmdLetterDate>
    <MaxAmdLetterDate>06/09/2008</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0712764</AwardID>
    <Investigator>
      <FirstName>Hal</FirstName>
      <LastName>Daume</LastName>
      <EmailAddress>hal@umiacs.umd.edu</EmailAddress>
      <StartDate>08/16/2007</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Utah</Name>
      <CityName>SALT LAKE CITY</CityName>
      <ZipCode>841128930</ZipCode>
      <PhoneNumber>8015816903</PhoneNumber>
      <StreetAddress>75 S 2000 E</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Utah</StateName>
      <StateCode>UT</StateCode>
    </Institution>
  </Award>
</rootTag>

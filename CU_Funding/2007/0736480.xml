<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>SGER: A Music Search Engine Based on Aesthetic Similarity</AwardTitle>
    <AwardEffectiveDate>08/01/2007</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2010</AwardExpirationDate>
    <AwardAmount>99564</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Maria Zemankova</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Abstract&lt;br/&gt;&lt;br/&gt;IIS - 0736480&lt;br/&gt;Manaris, Bill Z.&lt;br/&gt;College of Charleston&lt;br/&gt;A Music Search Engine based on Aesthetic Similarity&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This SGER project aims to develop a prototype music search engine based on identifying aesthetic similarities. This engine will utilize power-law metrics to extract statistical proportions of music-theoretic and other attributes of music pieces (e.g., Pitch, Duration, Pitch Distance, Duration Distance, Melodic Intervals, Harmonic Intervals, Melodic Bigrams, etc.).&lt;br/&gt;The engine searches for pieces that are aesthetically similar to the input piece using a mean squared error (MSE) approach. Preliminary testing has been done using the Classical Music Archives corpus (14,695 MIDI pieces), combined with 500+ MIDI pieces from other styles (e.g. Jazz, Rock, Country, etc.). Similar metrics have already been validated on aesthetic attributes of textual materials. Text results (author attribution, style identification, and pleasantness prediction) indicated an high level of accuracy. Assessment and validation experiments will be conducted to compare to computational findings indicating aesthetic similarity of retrieved pieces. These experiments will be conducted by Prof. Dwight Krehbiel (subaward, Bethel College), a specialist in cognitive neuroscience and psychology of music, who has extensive experience in measuring emotional and physiological responses to music.</AbstractNarration>
    <MinAmdLetterDate>08/06/2007</MinAmdLetterDate>
    <MaxAmdLetterDate>12/08/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0736480</AwardID>
    <Investigator>
      <FirstName>Dwight</FirstName>
      <LastName>Krehbiel</LastName>
      <EmailAddress>krehbiel@bethelks.edu</EmailAddress>
      <StartDate>08/06/2007</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Bill</FirstName>
      <LastName>Manaris</LastName>
      <EmailAddress>manarisb@cofc.edu</EmailAddress>
      <StartDate>08/06/2007</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>College of Charleston</Name>
      <CityName>CHARLESTON</CityName>
      <ZipCode>294240001</ZipCode>
      <PhoneNumber>8439534973</PhoneNumber>
      <StreetAddress>66 GEORGE ST</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>South Carolina</StateName>
      <StateCode>SC</StateCode>
    </Institution>
  </Award>
</rootTag>

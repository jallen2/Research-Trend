<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>New Methodology for Multiple Testing and Simultaneous Inference</AwardTitle>
    <AwardEffectiveDate>07/01/2007</AwardEffectiveDate>
    <AwardExpirationDate>12/31/2011</AwardExpirationDate>
    <AwardAmount>262402</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040000</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Gabor J. Szekely</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The investigator develops new methods and theory for problems in multiple testing and simultaneous inference. A classical approach to dealing with multiplicity is to require that decision rules control the familywise error rate. But, when the number of tests is large, this measure is so stringent that alternative hypotheses have little chance of being detected. Thus, alternative measures of error control are studied both in finite sample and asymptotically. Such measures include: the false discovery rate; the probability of k or more false rejections; tail probabilities of the false discovery proportion. In order to develop methods which do not rely on unrealistic or unverifiable model assumptions, the investigator makes extensive use of computer-intensive methods. The power of resampling is that the joint dependence structure of the individual tests can be captured so that methods need not be overly conservative. The pursuit of such methodology is investigated from theoretical, computational and theoretical points of view, with special emphasis on a large number of tests.&lt;br/&gt;&lt;br/&gt;The goal of this research is to develop new theory and methods for problems of multiple inference. Virtually any scientific experiment sets out to answer questions about the process under investigation, which often can be translated formally into a set of hypotheses to be tested. It is the exception that only a single hypothesis or question is under study. In the "information age", the statistician is faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any interesting conclusions can reliably be viewed as real structure rather than the result of "data snooping", i.e. finding artifacts of random data. For example, current methods in biotechnology and genomics generate DNA microarray experiments, where gene expression level in cells for thousands of genes are analyzed simultaneously on a gene by gene basis. The goal then is to devise new techniques that are not based on strong assumptions that effectively deal with problems of multiplicity in the face of vast amounts of data. The resulting inferential tools can be applied to such diverse fields as genetics, econometrics, finance, brain imaging, clinical trials, education and astronomy.</AbstractNarration>
    <MinAmdLetterDate>06/06/2007</MinAmdLetterDate>
    <MaxAmdLetterDate>05/08/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0707085</AwardID>
    <Investigator>
      <FirstName>Joseph</FirstName>
      <LastName>Romano</LastName>
      <EmailAddress>romano@stanford.edu</EmailAddress>
      <StartDate>06/06/2007</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Stanford University</Name>
      <CityName>Palo Alto</CityName>
      <ZipCode>943041212</ZipCode>
      <PhoneNumber>6507232300</PhoneNumber>
      <StreetAddress>3160 Porter Drive</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0000099</Code>
      <Name>Other Applications NEC</Name>
    </FoaInformation>
  </Award>
</rootTag>

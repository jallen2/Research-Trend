<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: AF: Small: Deep Learning Theory</AwardTitle>
    <AwardEffectiveDate>07/01/2016</AwardEffectiveDate>
    <AwardExpirationDate>06/30/2019</AwardExpirationDate>
    <AwardAmount>490000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Hector Munoz-Avila</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Deep learning has recently emerged as a major advance in machine learning and AI. This technology for learning from data has provided field-changing performance improvements in image classification and speech recognition, it has displayed impressive performance across a large variety of areas (including natural language processing, robotics, audio processing, and computational chemistry), and it has become a central ingredient in AI systems. But despite these successes, our understanding of these methods is incomplete. The broad goal of this research project is to address this grand challenge: to develop analysis techniques that enable us to understand when and why deep learning methods will be successful, and to design effective methods with explicit performance guarantees. Successful research outcomes have a significant potential for practical impact in the large and growing set of application areas where these methods are used.&lt;br/&gt;&lt;br/&gt;The project aims to understand the performance of deep learning methods - in particular to elucidate what aspects are essential for their success - and hence to develop principled design techniques and performance guarantees. The objectives are: to characterize the performance impacts of the critical features of current neural network architectures: scale, depth, nonlinearities, and regularization; to develop analysis techniques that facilitate our understanding of the approximation and estimation properties of deep architectures; to identify the boundary between easy and hard learning problems for deep networks; and to develop methods with explicit performance guarantees for optimization in deep neural networks. Successful research outcomes are likely to increase our understanding of deep learning methods, to provide performance guarantees for these methods, and to facilitate the principled design of novel deep learning methods.</AbstractNarration>
    <MinAmdLetterDate>06/10/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>06/10/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1619362</AwardID>
    <Investigator>
      <FirstName>Peter</FirstName>
      <LastName>Bartlett</LastName>
      <EmailAddress>bartlett@stat.berkeley.edu</EmailAddress>
      <StartDate>06/10/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Berkeley</Name>
      <CityName>BERKELEY</CityName>
      <ZipCode>947045940</ZipCode>
      <PhoneNumber>5106428109</PhoneNumber>
      <StreetAddress>Sponsored Projects Office</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7796</Code>
      <Text>ALGORITHMIC FOUNDATIONS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7926</Code>
      <Text>ALGORITHMS</Text>
    </ProgramReference>
  </Award>
</rootTag>

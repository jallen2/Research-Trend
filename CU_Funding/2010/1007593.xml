<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Conditional Modeling and Conditional Inference</AwardTitle>
    <AwardEffectiveDate>09/15/2010</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2013</AwardExpirationDate>
    <AwardAmount>245999</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040000</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Gabor J. Szekely</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>In many applications, the complexity and dimensionality of the data preclude nonparametric inference, despite the availability of massive data sets. At the same time, it is usually true that too little is known about the detailed mechanisms generating the data to meaningfully specify parametric models. Conditional modeling and conditional inference are semi-parametric approaches to complex high-dimensional data in which attention is focused on manageable low-dimensional statistical modeling and estimation. Applications include efficient feature estimation and data classification (e.g. in computer vision), exact tests for broad and scientifically relevant hypotheses (e.g. in the statistical analysis of multi-electrode neuronal recordings), exploration of time scale in non-stationary processes (e.g. in the study of market dynamics), and construction of complex distributions through successive low-dimensional perturbations (e.g. in the study of probabilistic context-sensitive grammars). &lt;br/&gt;&lt;br/&gt;High-dimensional data are ubiquitous. Sources include molecular biology, finance, neurophysiological recordings, and the imagery and text of the Internet. Despite the availability of almost unlimited amounts of these data, their complexity and high dimensionality challenge existing statistical models and represent a bottleneck to successful applications. Oftentimes the complexity and dimensionality can be finessed through mathematical methods that select and focus on a collection of low-dimensional characteristics of the data. The approach avoids untenable or un-testable model assumptions without necessarily compromising the information content and power of the data. The research is at the interface between statistical theory and scientific application, with potential impact in technology (e.g. through computer vision) and, more broadly, society (e.g. through neuroscience and better financial modeling).</AbstractNarration>
    <MinAmdLetterDate>09/09/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>09/09/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1007593</AwardID>
    <Investigator>
      <FirstName>Stuart</FirstName>
      <LastName>Geman</LastName>
      <EmailAddress>Stuart_Geman@Brown.edu</EmailAddress>
      <StartDate>09/09/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Matthew</FirstName>
      <LastName>Harrison</LastName>
      <EmailAddress>Matthew_Harrison@brown.edu</EmailAddress>
      <StartDate>09/09/2010</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Brown University</Name>
      <CityName>Providence</CityName>
      <ZipCode>029129002</ZipCode>
      <PhoneNumber>4018632777</PhoneNumber>
      <StreetAddress>BOX 1929</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Rhode Island</StateName>
      <StateCode>RI</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1269</Code>
      <Text>STATISTICS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>9150</Code>
      <Text>EXP PROG TO STIM COMP RES</Text>
    </ProgramReference>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>EAGER: Modeling and Recognizing Collective Activities</AwardTitle>
    <AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2012</AwardExpirationDate>
    <AwardAmount>80000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jie Yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project explores a novel principled framework for learning generic models of collective activities. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models are used, in turn, for detecting, classifying, and segmenting activities as well as indentifying activities that differ from the collective behavior from videos sequences. Research developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals in isolation. Furthermore, unlike many current contributions, the aim is to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras.&lt;br/&gt;&lt;br/&gt;Key intellectual contributions of this project are: i) a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks using context (e.g., scene and object recognition); ii) a methodology based on Relational Dependency Networks for segmenting different collective activities and discovering anomalous ones. &lt;br/&gt;&lt;br/&gt;This project can provides critical building blocks toward addressing high level visual problems such as modeling the interaction between humans/animals and objects, constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has a potential to play a transformative role in strategic areas such as robotics and navigation. It also provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).</AbstractNarration>
    <MinAmdLetterDate>08/06/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>08/06/2010</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1052762</AwardID>
    <Investigator>
      <FirstName>Silvio</FirstName>
      <LastName>Savarese</LastName>
      <EmailAddress>ssilvio@stanford.edu</EmailAddress>
      <StartDate>08/06/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Michigan Ann Arbor</Name>
      <CityName>Ann Arbor</CityName>
      <ZipCode>481091274</ZipCode>
      <PhoneNumber>7347636438</PhoneNumber>
      <StreetAddress>3003 South State St. Room 1062</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Michigan</StateName>
      <StateCode>MI</StateCode>
    </Institution>
  </Award>
</rootTag>

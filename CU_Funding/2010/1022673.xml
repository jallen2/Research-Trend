<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Collaborative Research: Automated Analysis of Constructed Response Concept Inventories to Reveal Student Thinking: Forging a National Network for Innovative Assessment Methods</AwardTitle>
    <AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2014</AwardExpirationDate>
    <AwardAmount>84529</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>11040200</Code>
      <Directorate>
        <LongName>Direct For Education and Human Resources</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Undergraduate Education</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Myles G. Boylan</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Numerous reports on the effectiveness of U.S. higher education in the Science, Technology, Engineering and Mathematics (STEM) disciplines call for increased emphasis on conceptual learning, rather than rote memorization. Suitable assessments (tests) of conceptual learning (often referred to as concept inventories or diagnostic question clusters), however, are few and are constrained by the ability to score the outcomes of the tests in a cost-effective manner. Multiple-choice assessments (selected responses) are more widespread in higher education, especially at medium to large institutions where class sizes are large, and where automated scoring provides the essential cost-effectiveness. Conversely, written response assessments (constructed responses), which are widely held to be superior at revealing actual student thinking, are quite rare in practice given the time and effort required for manual scoring. This project leverages the latest computerized tools and statistical techniques to make constructed response assessments more broadly available. Computer automation allows the use of these more insightful conceptual questions and tests with much larger numbers of students, thereby providing an enhanced understanding of students' conceptual learning. Project personnel work with developers of conceptual testing instruments to create constructed response versions of the tests coupled with the necessary computerized scoring tools with the eventual goal of providing computer-automated evaluation of conceptual thinking. The project is a collaboration among three major public universities.</AbstractNarration>
    <MinAmdLetterDate>09/06/2010</MinAmdLetterDate>
    <MaxAmdLetterDate>08/21/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1022673</AwardID>
    <Investigator>
      <FirstName>Michelle</FirstName>
      <LastName>Smith</LastName>
      <EmailAddress>michelle.k.smith@colorado.edu</EmailAddress>
      <StartDate>09/06/2010</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Jennifer</FirstName>
      <LastName>Knight</LastName>
      <EmailAddress>jennifer.knight@colorado.edu</EmailAddress>
      <StartDate>09/06/2010</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Colorado at Boulder</Name>
      <CityName>Boulder</CityName>
      <ZipCode>803031058</ZipCode>
      <PhoneNumber>3034926221</PhoneNumber>
      <StreetAddress>3100 Marine Street, Room 481</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Colorado</StateName>
      <StateCode>CO</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1536</Code>
      <Text>S-STEM:SCHLR SCI TECH ENG&amp;MATH</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>7492</Code>
      <Text>CCLI-Type 2 (Expansion)</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>9178</Code>
      <Text>UNDERGRADUATE EDUCATION</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>SMET</Code>
      <Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>

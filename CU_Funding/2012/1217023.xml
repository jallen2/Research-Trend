<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CIF: Small: Fast Rate-Efficient Codes for Data Compression and Transmission via Sparse Regression</AwardTitle>
    <AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2016</AwardExpirationDate>
    <AwardAmount>499498</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Richard Brown</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Modern communication networks are constantly growing in size and sophistication. New applications require these networks to be reliable, computationally efficient, and have small latency. To meet these demands it is critical to have low-complexity, rate-efficient codes for communication and compression. Since Shannon's fundamental results, there has been a flurry of activity to design capacity achieving, computationally efficient coding schemes. For the channel coding problem, it was not until the early 1990s that capacity achieving codes were implemented. Similarly, many good quantizer designs were developed for lossy compression, but unfortunately none of these low-complexity codes provably attain the rate-distortion bound. The divergence between information-theoretic results and code construction is even more pronounced in network communication problems. Despite a sharp characterization of information-theoretic limits for several network models, the best practical codes for these problems fall short of the capacity limits.&lt;br/&gt;&lt;br/&gt;This research involves the development of computationally efficient codes for Gaussian sources and channels. To this end we leverage recent advances in high-dimensional sparse regression. These are the first low-complexity codes that provably attain the information-theoretic limits codes for Gaussian sources and channels. The main objectives of this research project are: (1) Determining the fundamental limits of sparse regression codes in a variety of communication theoretic settings; (2) Developing low-complexity encoding and decoding schemes for our sparse regression codes. This part of the project draws on ideas from function approximation and sparse signal recovery. The project also provides an opportunity for training graduate students and postdoctoral researchers in the disciplines of communication theory, data compression, statistics and networks.</AbstractNarration>
    <MinAmdLetterDate>08/27/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>08/27/2012</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1217023</AwardID>
    <Investigator>
      <FirstName>Sekhar</FirstName>
      <LastName>Tatikonda</LastName>
      <EmailAddress>sekhar.tatikonda@yale.edu</EmailAddress>
      <StartDate>08/27/2012</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Yale University</Name>
      <CityName>New Haven</CityName>
      <ZipCode>065208327</ZipCode>
      <PhoneNumber>2037854689</PhoneNumber>
      <StreetAddress>Office of Sponsored Projects</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Connecticut</StateName>
      <StateCode>CT</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7797</Code>
      <Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7935</Code>
      <Text>COMM &amp; INFORMATION THEORY</Text>
    </ProgramReference>
  </Award>
</rootTag>

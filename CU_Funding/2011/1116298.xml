<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>AF: Small: Parallel Global Optimization Algorithms with Asynchrony, Adaptive Re-Planning, and Response Surfaces for Costly Simulations</AwardTitle>
    <AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2016</AwardExpirationDate>
    <AwardAmount>499867</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Jack Snoeyink</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Optimization can be used for designing lower cost or more efficient strategies and selecting model parameters so that models can accurately predict system behavior. However, for computationally expensive simulation models, conventional optimization can require infeasibly long computation times (months or years) The focus of this research is to develop a method (called POARS) to use parallel processors to find the global minimum of a function with relatively few simulations. Examples of such computationally expensive simulation models are systems of nonlinear partial differential equations used to describe fluid flow and reactive chemistry transport that arise in many applications including groundwater and atmospheric modeling. Because such models can take hours or even days to complete a single simulation, it is important to have algorithms that find good optimal solutions with few simulations, which the proposed research achieves by integrating surrogate response surfaces into the optimization. The algorithm will be applied to real models (based on field data) including a groundwater model (partial differential equations) and two watershed models (spatially distributed difference equations). &lt;br/&gt;&lt;br/&gt;The POARS algorithm incorporates multiple novel features including Ensembles, Asynchronous Parallelism with Adaptive Re-planning and the use of multiple optimization methods on the response surface, where many evaluations can be made cheaply. The asynchronous aspect enables new simulations to start before simulations on other parallel processors are complete. The response surface is based on an ensemble of response surfaces including mixture models based on Dempster-Schafer theory. A proof of almost sure convergence of POARS will be given. The research will couple the optimization method on M processes with parallel simulations each on N processes to efficiently use MN processors.</AbstractNarration>
    <MinAmdLetterDate>08/24/2011</MinAmdLetterDate>
    <MaxAmdLetterDate>08/24/2011</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1116298</AwardID>
    <Investigator>
      <FirstName>Christine</FirstName>
      <LastName>Shoemaker</LastName>
      <EmailAddress>cas12@cornell.edu</EmailAddress>
      <StartDate>08/24/2011</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Cornell University</Name>
      <CityName>Ithaca</CityName>
      <ZipCode>148502820</ZipCode>
      <PhoneNumber>6072555014</PhoneNumber>
      <StreetAddress>373 Pine Tree Road</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7796</Code>
      <Text>ALGORITHMIC FOUNDATIONS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7934</Code>
      <Text>PARAL/DISTRIBUTED ALGORITHMS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9218</Code>
      <Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>HPCC</Code>
      <Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
    </ProgramReference>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Gaze Control during Scene Viewing: Behavioral and Computational Approaches</AwardTitle>
    <AwardEffectiveDate>02/01/2012</AwardEffectiveDate>
    <AwardExpirationDate>05/31/2016</AwardExpirationDate>
    <AwardAmount>414422</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Catherine Arrington</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>When we view the visual world, our eyes flit from one location to another about three times per second, in movements called saccades. Useful visual information is acquired only during fixations, brief periods of time when gaze rests on an object or scene feature. The cognitive and neural processes that direct saccades and fixations through a scene in real time fall under the term 'gaze control'. This project focuses on unraveling how human gaze control operates during active real-world scene perception.&lt;br/&gt;&lt;br/&gt;This project approaches human gaze control by starting with the insight that understanding eye movement timing will provide key insight into the underlying cognitive and neural systems that control gaze. The research combines innovative eye-tracking methods with a working computational model that simulates eye movement control. In this research program, the empirical and computational threads are complementary and synergistic. On the one hand, we can test our understanding of gaze control by determining whether the model can produce eye movements that look like those produced by people. On the other hand, insights from the model can be used as a tool to enhance our theoretical understanding of gaze control, and these insights can be further tested with new experiments.&lt;br/&gt;&lt;br/&gt;The results from this project will enhance basic scientific understanding of how humans perceive and understand the visual world. The project also has wide-ranging implications for the creation of new display technologies and machine interfaces that can be controlled by eye movements. And the results are relevant for the design of new artificial vision systems that actively track and focus on relevant information in the environment.</AbstractNarration>
    <MinAmdLetterDate>01/31/2012</MinAmdLetterDate>
    <MaxAmdLetterDate>04/27/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1151358</AwardID>
    <Investigator>
      <FirstName>John</FirstName>
      <LastName>Henderson</LastName>
      <EmailAddress>johnhenderson@ucdavis.edu</EmailAddress>
      <StartDate>01/31/2012</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University South Carolina Research Foundation</Name>
      <CityName>COLUMBIA</CityName>
      <ZipCode>292080001</ZipCode>
      <PhoneNumber>8037777093</PhoneNumber>
      <StreetAddress>1600 Hampton Street</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>South Carolina</StateName>
      <StateCode>SC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>9150</Code>
      <Text>EXP PROG TO STIM COMP RES</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>9150</Code>
      <Text>EXP PROG TO STIM COMP RES</Text>
    </ProgramReference>
  </Award>
</rootTag>

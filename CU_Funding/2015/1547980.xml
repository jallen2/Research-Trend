<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Architecture Support for Advancing PGAS (ASAP)</AwardTitle>
    <AwardEffectiveDate>08/15/2015</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2017</AwardExpirationDate>
    <AwardAmount>229983</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Almadena Y. Chtchelkanova</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Architectures of parallel computers and modern manycore processor chips, that contain tens and eventually hundreds of processors, are becoming quite complex for the programmers. Efficient programming of those systems is very important to achieve high-speed of processing while reducing power. To do so, programmers must ensure that: 1. the work in the program is broken into as many parallel activities as possible for fast processing; 2. data is located close to the processing cores that will manipulate them to avoid making the data travel long in the system thereby losing more time and wasting energy. Programming models are abstractions that provide the programmer with an easy-to-use logical view that hides the complexity of the underlying systems, while facilitating efficient programming. These are two conflicting requirements and while the current de facto programming methods can offer efficiency in the majority of the cases, they are not easy to use. The so called, PGAS or the Partitioned Global Address Space programming model has the promise of striking a balance between efficiency and ease-of-use. However, help is needed from the hardware particularly in simplifying and speeding up the process of finding where the data to be processed is physically located. This work is to investigate hardware solutions for this problem under the PGAS programming model. The outcome will improve productivity of domain scientists, thereby reducing the time from conceiving an application problem till the solution is attained, which in the long run can mean more rapid discoveries and innovations, as well reduction in the cost of developing the next generation software. &lt;br/&gt;The PIs propose to investigate a general hardware support for address translation for the PGAS programming model. PGAS strikes a balance between the locality-aware, but explicit, message-passing model (e.g. MPI) and the easy-to-use, but locality-agnostic, shared memory model (e.g. OpenMP). However, the PGAS rich memory model comes at a performance cost which can hinder its potential for scalability and performance. Current implementations can be orders of magnitude slower in accessing local shared space as compared to accessing their private space. Compiler optimizations only handle special cases and hand-tuning renders the PGAS ease-of-use advantage worthless. The proposed hardware solution can facilitate high-performance execution for out-of-the-box (i.e. non-hand-tuned) PGAS applications. The PIs are creating PAGS memory model translation architectural support, which can navigate the PGAS memory model converting PGAS shared references to system's virtual addresses efficiently on-the-fly. This eliminates the need for hand-tuning, while maintaining the performance and productivity of PGAS languages. The hardware support will be available to the compiler through instruction set extensions. A tool set integrating and adapting existing micro-architecture simulators with compiler and a run-time system will be used as the main testbed and distributed over a cluster for extensive experimentation. At the end of the project the PIs expect to release tools and the benchmarks utilized under this project.</AbstractNarration>
    <MinAmdLetterDate>08/05/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>08/05/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1547980</AwardID>
    <Investigator>
      <FirstName>Tarek</FirstName>
      <LastName>El-Ghazawi</LastName>
      <EmailAddress>tarek@gwu.edu</EmailAddress>
      <StartDate>08/05/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>George Washington University</Name>
      <CityName>Washington</CityName>
      <ZipCode>200522000</ZipCode>
      <PhoneNumber>2029946255</PhoneNumber>
      <StreetAddress>2121 Eye Street NW</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>District of Columbia</StateName>
      <StateCode>DC</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7798</Code>
      <Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7916</Code>
      <Text>EAGER</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7942</Code>
      <Text>HIGH-PERFORMANCE COMPUTING</Text>
    </ProgramReference>
  </Award>
</rootTag>

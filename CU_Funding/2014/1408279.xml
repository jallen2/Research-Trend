<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Collaborative Research: Fast reinforcement learning using multiple models and state decompositions for apllications to Plug-in Hybrid Vehicles</AwardTitle>
    <AwardEffectiveDate>08/15/2014</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2017</AwardExpirationDate>
    <AwardAmount>200000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07010000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Usha Varshney</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Learning schemes in general, are known for their slow convergence rates. The proposal suggests various modifications for improving the speed of learning. The first is the use of multiple models which has proved very effective in other areas of systems theory. A second modification is the use of decentralized or partially decentralized multi-agent state decomposition approaches. Both of the above will make learning schemes in general and reinforcement schemes in particular fast converging and applicable to a wide class of real-world problems. The proposed research will develop the foundations of fast reinforcement learning using these two broad approaches, and will also apply these methods to the optimal control of a fleet of Plug-in Hybrid Electric Vehicles. The outcome of this research will have a societal impact for optimally controlling a fleet of Plug-in Hybrid Electric Vehicles for energy efficiency. The application of the proposed research will have extended application in other areas of systems theory such as neurobiology. Graduate and undergraduate students will be trained in learning schemes for multidisciplinary applications. Cross-fertilization of ideas will be facilitated through a bi-annual workshop on adaptive and learning systems and by leveraging a multidisciplinary Institute for Mathematical Modeling and Computational Science (iMMCS).&lt;br/&gt;&lt;br/&gt;The proposal focuses on improving convergence speed of learning schemes in multiple agents in order to overcome the limitations of dimensionality, as well as in situations where infrequent communication exists between agents. Innovation is in the use of multiple models and state decomposition to speed up the learning and convergence. The technical approach lies in using multiple identification models; reinforcement learning by using decentralized or partially decentralized multi-agent state decomposition approaches; and evaluating fast reinforcement learning in an application test-bed for controlling a fleet of Plug-in Hybrid Electric Vehicles (PHEVs). The Research Team further plans to quantify the trade-off between learning speed and quality of the "learned solutions".</AbstractNarration>
    <MinAmdLetterDate>08/04/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>08/04/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1408279</AwardID>
    <Investigator>
      <FirstName>Kumpati</FirstName>
      <LastName>Narendra</LastName>
      <EmailAddress>kumpati.narendra@yale.edu</EmailAddress>
      <StartDate>08/04/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Yale University</Name>
      <CityName>New Haven</CityName>
      <ZipCode>065208327</ZipCode>
      <PhoneNumber>2037854689</PhoneNumber>
      <StreetAddress>Office of Sponsored Projects</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Connecticut</StateName>
      <StateCode>CT</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7607</Code>
      <Text>ENERGY,POWER,ADAPTIVE SYS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1653</Code>
      <Text>Adaptive &amp; intelligent systems</Text>
    </ProgramReference>
  </Award>
</rootTag>

<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>EAPSI: Understanding attention during the early stages of language acquisition using a new tracking eye gaze method</AwardTitle>
    <AwardEffectiveDate>06/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>05/31/2015</AwardExpirationDate>
    <AwardAmount>5070</AwardAmount>
    <AwardInstrument>
      <Value>Fellowship</Value>
    </AwardInstrument>
    <Organization>
      <Code>01090000</Code>
      <Directorate>
        <LongName>Office Of The Director</LongName>
      </Directorate>
      <Division>
        <LongName>Office Of Internatl Science &amp;Engineering</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Anne L. Emig</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>When it comes to knowing how children learn their first words, understanding how visual attention is guided is especially important. Objects being learned can vary in many ways when taking a child's view of the world. When exposed to human speech, children have the option to gaze upon a wide range of items, some of which are potentially informative for grounding words (auditory information) to physical things they see (visual information). This project will apply cutting edge techniques used for study where children look when they learn. This research aims to develop a solution for the problems we encounter in child-directed eye tracking research. Dr. Nishida at Kyoto University, Japan, has the knowledge and expertise to help with achieving this goal by implementing a promising technique for point of gaze estimation.&lt;br/&gt;&lt;br/&gt;Current methods for observing how attention is focused during learning typically involve the use of eye gaze tracking under experimental conditions. Currently, multiple hardware options exist for accomplishing this task, but have their own unique set of advantages and disadvantages. Mobile, wearable eye tracking solutions provide more flexibility in less constrained settings but require longer setup times, adjustments, and calibration. This increases the overall intrusiveness of the experimental procedure and often leads to the child being aware of such devices. Eye-tracking hardware not attached to the child (e.g., a specialized video monitor) typically have more precise measurements of gaze orientation but lack the ability to measure gaze in three-dimensional environments. The new method that will be used in this project, analyzes corneal reflections using computer vision algorithms on super resolution eye images to better accommodate child behavioral tasks. It has the potential to significantly reduce the complexities involved in obtaining accurate and precise data under depth-varying conditions for understanding the role of attention during the early stages of the language learning process. This NSF EAPSI award is funded in collaboration with the Japan Society for the Promotion of Science.</AbstractNarration>
    <MinAmdLetterDate>06/04/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>06/04/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1414826</AwardID>
    <Investigator>
      <FirstName>Joseph</FirstName>
      <LastName>Burling</LastName>
      <EmailAddress/>
      <StartDate>06/04/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Burling Joseph M</Name>
      <CityName>Houston</CityName>
      <ZipCode>770434232</ZipCode>
      <PhoneNumber/>
      <StreetAddress/>
      <CountryName>United States</CountryName>
      <StateName>Texas</StateName>
      <StateCode>TX</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7316</Code>
      <Text>EAPSI</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>5921</Code>
      <Text>JAPAN</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>5978</Code>
      <Text>EAST ASIA AND PACIFIC PROGRAM</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7316</Code>
      <Text>EAPSI</Text>
    </ProgramReference>
  </Award>
</rootTag>

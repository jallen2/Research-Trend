<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>RI: Small: Language Induction meets Language Documentation: Leveraging bilingual aligned audio for learning and preserving languages</AwardTitle>
    <AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>11/30/2014</AwardExpirationDate>
    <AwardAmount>470000</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Thousands of the world's languages are in danger of dying out before they have been systematically documented. Many other languages have millions of speakers, yet they exist only in spoken form, and minimal documentary records are available. As a consequence, important sources of knowledge about human language and culture are inaccessible, and at risk of being lost forever. Moreover, it is difficult to develop technologies for processing these languages, leaving their speech communities on the far side of a widening digital divide. The first step to solving these problems is language documentation, and so the goal of this project is to develop computational methods based on automatic speech recognition and machine translation for documenting endangered and unwritten languages on an unprecedented scale.&lt;br/&gt;&lt;br/&gt;To be successful, any approach must guarantee both the sufficiency and interpretability of the documentation it produces. This project ensures sufficiency by using a combination of community outreach, crowdsourcing techniques, and mobile/web technologies to collect hundreds of hours (millions of words) of speech. The interpretability is enabled by augmenting original speech recordings with careful verbatim repetitions along with translations into a well-resourced language. Finally, computational models are developed to automate transcription of recordings and alignment with translations, resulting in bilingual aligned text. The result is a kind of digital Rosetta Stone: a large-scale key for interpreting the world's languages even if they are not written, or no longer even spoken.</AbstractNarration>
    <MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>09/05/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1423406</AwardID>
    <Investigator>
      <FirstName>Steven</FirstName>
      <LastName>Bird</LastName>
      <EmailAddress>sbird@unimelb.edu.au</EmailAddress>
      <StartDate>08/18/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>David</FirstName>
      <LastName>Chiang</LastName>
      <EmailAddress>dchiang@nd.edu</EmailAddress>
      <StartDate>08/18/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Southern California</Name>
      <CityName>Los Angeles</CityName>
      <ZipCode>900890001</ZipCode>
      <PhoneNumber>2137407762</PhoneNumber>
      <StreetAddress>University Park</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
  </Award>
</rootTag>

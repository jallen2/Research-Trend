<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Large-Scale Structured Optimization: Convex Relaxations and Gradient Methods</AwardTitle>
    <AwardEffectiveDate>09/01/2009</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2012</AwardExpirationDate>
    <AwardAmount>0</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040100</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Rosemary Renaut</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Optimization problems arise in signal/image denoising,&lt;br/&gt;compressed sensing, sensor network localization, multi-task learning, data&lt;br/&gt;mining/classification, are large-scale, nonlinear, but highly structured.&lt;br/&gt;An effective solution approach is to approximate the problems by&lt;br/&gt;convex relaxations that are computationally tractable and inherit the&lt;br/&gt;structures of the original problems. Specialized computer algorithms&lt;br/&gt;are then developed to exploit the structures and&lt;br/&gt;efficiently solve the convex relaxations in real time.&lt;br/&gt;The investigator and his colleagues/students study the accuracy of&lt;br/&gt;different convex relaxations (i.e., how well they approximate the&lt;br/&gt;original problems) when data may be noisy, and develop new computer&lt;br/&gt;algorithms that are more efficient in theory and in practice.&lt;br/&gt;This includes coordinatewise and incremental gradient algorithms,&lt;br/&gt;possibly accelerated through extrapolation.&lt;br/&gt;Complexity and convergence rate are studied, as well as&lt;br/&gt;computer implementation and testing.&lt;br/&gt;Thus the twin topics of approximation by convex relaxations and&lt;br/&gt;algorithms for convex relaxations are integrated in their development.&lt;br/&gt;&lt;br/&gt;Measured data, from satellite images to biological images to stock indices&lt;br/&gt;to internet queries/usage, are inherently noisy and large size.&lt;br/&gt;How to process such large noisy data and extract key&lt;br/&gt;features/patterns is a major challenge, with important applications to&lt;br/&gt;image restoration/denoising, economic forecasting, pattern recognition,&lt;br/&gt;data classification, etc. In the proposed research, parametrized&lt;br/&gt;mathematical models of the underlying data generating process are&lt;br/&gt;formulated and the parameters are tuned by specialized computer algorithms&lt;br/&gt;to optimize the model's predictive power. The latter is achieved&lt;br/&gt;by optimizing a combination of the model's goodness-of-fit to data and a&lt;br/&gt;measure of model's simplicity. This is motivated by Occam's Razor&lt;br/&gt;principle that the simplest is the best (in particular, for identifying&lt;br/&gt;underlying patterns and trends). For example, a 1 Mega-pixel&lt;br/&gt;noisy image might be modeled by a weighted sum of 1000 "elementary"&lt;br/&gt;images from a dictionary. The computer algorithm then finds weights&lt;br/&gt;that are sparse (i.e., few nonzeros) and&lt;br/&gt; fit the model closely to the noisy image (say, in a least squares&lt;br/&gt;sense of the pixel values). Predicting&lt;br/&gt;people's behavior based on past data is another&lt;br/&gt;example, such as the Netflix prize for predicting movie rating which&lt;br/&gt;has a training data set of over 100 million ratings from over 480 thousand&lt;br/&gt;people. Owing to the large data size (and, in some cases, a need to&lt;br/&gt;process the data fast and in real time) and possibly high-dimensional&lt;br/&gt;parameter space, specialized algorithms based on new techniques (e.g.,&lt;br/&gt;work with small chunks of data or small number of parameters at each time)&lt;br/&gt;need to be developed. This is the aim of the proposed research.</AbstractNarration>
    <MinAmdLetterDate>08/18/2009</MinAmdLetterDate>
    <MaxAmdLetterDate>09/16/2009</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0914215</AwardID>
    <Investigator>
      <FirstName>Paul</FirstName>
      <LastName>Tseng</LastName>
      <EmailAddress>tseng@math.washington.edu</EmailAddress>
      <StartDate>08/18/2009</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Washington</Name>
      <CityName>Seattle</CityName>
      <ZipCode>981950001</ZipCode>
      <PhoneNumber>2065434043</PhoneNumber>
      <StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Washington</StateName>
      <StateCode>WA</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0000099</Code>
      <Name>Other Applications NEC</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>1271</Code>
      <Text>COMPUTATIONAL MATHEMATICS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>0000</Code>
      <Text>UNASSIGNED</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9263</Code>
      <Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>OTHR</Code>
      <Text>OTHER RESEARCH OR EDUCATION</Text>
    </ProgramReference>
  </Award>
</rootTag>
